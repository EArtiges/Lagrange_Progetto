{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import gdbm\n",
    "mod = __import__('gdbm')\n",
    "import dumbdbm\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import datasets, decomposition\n",
    "import numpy as np\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "#datetime.strptime('2016-03-29','%Y-%d-%m')\n",
    "import dateparser\n",
    "from ipykernel import kernelapp as app\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from operator import itemgetter\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import Instalib as IL\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import pickle\n",
    "import scipy.io\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk.stem\n",
    "import sktensor\n",
    "import geopy.distance\n",
    "from itertools import groupby\n",
    "import ncp\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the file and have a first look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data and concatenate in a single DataFrame file, basic file format for pandas. It's a 2-D object, tabular file with labeled rows and columns, built from a dictionary or such. \n",
    "\n",
    "More info about it here: https://pandas.pydata.org/pandas-docs/stable/10min.html\n",
    "\n",
    "Make our life easier by matching the Torino labels with the Marseille ones and convert strings to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.read_csv('/data/marcoq/instagram/torino/torino_posts.csv',lineterminator='\\n')\n",
    "df_new=df_new.rename(columns = {'caption':'text','taken_at_timestamp':'date'})\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates, Histo = IL.N_posts(df_new, debug=False)\n",
    "Npostslog=[np.log10(H) for H in Histo]\n",
    "Nposts=[H for H in Histo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Figures' not in os.listdir('.') and False:\n",
    "    os.mkdir('Figures')\n",
    "    os.mkdir('Figures/Shape')\n",
    "    os.mkdir('Figures/Shape/Spatial')\n",
    "    os.mkdir('Figures/Shape/Temporal')\n",
    "plt.figure()\n",
    "plt.scatter(list_dates, Npostslog)\n",
    "plt.xticks(list_dates[0::15])\n",
    "plt.ylabel(r'$log_{10}$(#posts)')\n",
    "plt.xlabel('Timeline')\n",
    "plt.title(r'$log_{10}$ of the amount of insta posts per months')\n",
    "#plt.savefig('Figures/Shape/Temporal/Torino_logN.pdf')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.scatter(list_dates, Nposts)\n",
    "plt.xticks(list_dates[0::15])\n",
    "plt.ylabel('#posts)')\n",
    "plt.xlabel('Timeline')\n",
    "plt.title('Absolute amount of insta posts per months')\n",
    "#plt.savefig('Figures/Shape/Temporal/Torino_N.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location_all=pd.read_csv('/data/marcoq/instagram/torino/torino_locations.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location_all=df_location_all.rename(columns = {'latitude':'lat','longitude':'lon', 'location_id':'id'})\n",
    "df_location_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember zip:\n",
    "zip([1,2,3],['a','b','c'])\n",
    "#it's a function.\n",
    "#make a new column 'coord' indexing (lat,lon) for each post\n",
    "df_location_all['coord'] = list(zip(df_location_all.lat,df_location_all.lon))\n",
    "df_location_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap per year\n",
    "TbyCoord=IL.time_coord(df_location_all, df_new)\n",
    "for year in range(2010,2019,1):\n",
    "    Coordinates=[]\n",
    "    for t in TbyCoord:\n",
    "        if t[0][0:4]==str(year):\n",
    "            Coordinates.append(t[1])\n",
    "    IL.geo_heatmap(Coordinates, save=False,filename='hm'+str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's refine the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link postID with its location\n",
    "dict_id_coord= dict(zip(df_location_all.id,df_location_all.coord))\n",
    "dict_id_name= dict(zip(df_location_all.id,df_location_all.name))\n",
    "dict_coord_name= dict(zip(df_location_all.coord,df_location_all.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "series.map() matches corresponding entries from two different series files. This is what x.map(y) does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series([1,2,3], index=['one', 'two', 'three'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.Series(['foo', 'bar', 'baz'], index=[1,2,3])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.map(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a column 'coords' by mapping the coordinates to their location_id:\n",
    "df_new['coords'] = df_new['location_id'].map(dict_id_coord)\n",
    "df_new['name'] = df_new['location_id'].map(dict_id_name)\n",
    "\n",
    "#Create lat and lon columns by reading coords\n",
    "# Look here: https://pythonhow.com/accessing-dataframe-columns-rows-and-cells/\n",
    "df_new['lat']=[e[0] for e in df_new.loc[:,'coords']]\n",
    "df_new['lon']=[e[1] for e in df_new.loc[:,'coords']]\n",
    "\n",
    "# Drop duplicates\n",
    "df_new.drop_duplicates(keep='first')\n",
    "df_new.drop('location_id',axis=1,inplace=True)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs to be done at the end for compatibility reasons\n",
    "# learn how to parse time here: https://www.tutorialspoint.com/python/time_strptime.htm\n",
    "\n",
    "df_new['date']=[datetime.strptime(t,'%Y-%m-%d %H:%M:%S') for t in list(df_new['date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(df_new, open('df_new_chkpt1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the frame and the grid. $$x_{min/max}=longitude_{min/max}$$ and conversely for y and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp.reload(IL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attention problème d'arrondi lors du passage de Coord à (lat,lon)\n",
    "step_m=0.5\n",
    "lat1,lat2,step_lat,lon1,lon2,step_lon,to_bin_lon,to_bin_lat = IL.build_grid(df_new,step_m)\n",
    "number_lats,number_lon,df_new=IL.add_grid_todf(df_new,to_bin_lat,to_bin_lon,step_lat,step_lon)\n",
    "print (number_lats, number_lon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(df_new, open('df_new_chkpt2.pkl', 'wb'))\n",
    "#df_new=pd.read_pickle('df_new_chkpt2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to clean the text\n",
    "\n",
    "But first make a copy before we do anything stupid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_save=df_new.copy()\n",
    "#df_new=df_new_save.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purge all the newline characters\n",
    "df_new['text']=df_new.text.replace('\\n',' ',regex=True)\n",
    "print df_new['text'][22].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_new.text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This methods works only on unicode text. So we need to use the function x.decode('utf-8') to convert all strings in the 'text' field to unicode objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_0=df_new['text'][22][0:11].decode('utf-8')\n",
    "print text_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "#df_new['text_new2'] = df_new['text_new2'].map(lambda x: emoji_pattern.sub(r'', x))\n",
    "       \n",
    "#It works on unicode but not on a string:\n",
    "    \n",
    "text = u'This dog \\U0001f602'\n",
    "print(text) # with emoji\n",
    "print(emoji_pattern.sub(r'', text)) # no emoji\n",
    "\n",
    "text = 'This dog \\U0001f602'\n",
    "print(text) # with emoji\n",
    "print(emoji_pattern.sub(r'', text)) # no emoji\n",
    "\n",
    "print '\\n but some emojis still pass through... \\n'\n",
    "\n",
    "text=text_0\n",
    "print(text) # with emoji\n",
    "print(emoji_pattern.sub(r'', text)) # no emoji\n",
    "\n",
    "print(text) # with emoji\n",
    "print(emoji_pattern.sub(r'', text)) # no emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to deal with emoji would be to de-emojize them. They would then count as words, that can either be removed\n",
    "since they have the very peculiar following form :grinning_face: \n",
    "\n",
    "The difficulty here is identifying emojis in a text. Their classification is hard to decypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text) # with emoji\n",
    "print emoji.demojize(text) # no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all blank fields and encode everything in utf-8 if it is not already the case\n",
    "df_new=df_new[df_new['text'].notnull()]\n",
    "if not type(df_new['text'][0])==unicode:\n",
    "    df_new['text']=df_new['text'].map(lambda x: x.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove emojis? some of them still pass through\n",
    "#re.sub(a,b,string) = remplace le premier a par b dans string et renvoie string. \n",
    "#Si a n'est pas dans string, renvoie string.\n",
    "keys=emoji.unicode_codes.UNICODE_EMOJI.keys()\n",
    "df_new['text_new'] = df_new['text'].map(lambda x: emoji_pattern.sub(r'', x))\n",
    "#df_new['text_new'] = df_new['text'].map(lambda x: rem_emj(x,keys)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df_new[df_new['text'].notnull()]\n",
    "if not type(df_new['text'][0])==unicode:\n",
    "    df_new['text']=df_new['text'].map(lambda x: x.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hyperlinks\n",
    "df_new['text_new'] = df_new['text_new'].map(lambda x: re.sub(r'http\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove quotes\n",
    "df_new['text_new'] = df_new['text_new'].map(lambda x: re.sub(r'&amp;quot;|&amp;amp', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tags\n",
    "df_new['text_new'] = df_new['text_new'].map(lambda x: re.sub(r'@[a-zA-Z0-9]*', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tickers\n",
    "df_new['text_new']= df_new['text_new'].map(lambda x: re.sub(r'\\$[a-zA-Z0-9]*', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers\n",
    "df_new['text_new']=df_new['text_new'].map(lambda x:  re.sub(r'[0-9]*','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove #\n",
    "df_new['text_new']=df_new['text_new'].map(lambda x:  re.sub(r'#*','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation\n",
    "df_new.text_new = [filter(lambda x: x not in string.punctuation,tweet) for tweet in df_new.text_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print len(df_new.text_new)\n",
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding back in strings\n",
    "df_new['text_new'] = df_new['text_new'].map(lambda x: x.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df_new[df_new['text_new'].notnull()]\n",
    "df_new=df_new[df_new['text_new'] != '']\n",
    "df_new=df_new[['date','lat','lon','latbin','lonbin','coords','text_new','name']]\n",
    "df_new=df_new.rename(columns = {'text_new':'text'})\n",
    "df_new['coordsbin'] = list(zip(df_new.latbin,df_new.lonbin))\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=IL.text_processing(df_new)\n",
    "df_new=df_new[['date','lat','lon','latbin','lonbin','coords','text_new','name']]\n",
    "df_new=df_new.rename(columns = {'text_new':'text'})\n",
    "df_new['coordsbin'] = list(zip(df_new.latbin,df_new.lonbin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#And pickle it\n",
    "#df_new.to_pickle('df_new_chkpt3.pkl')\n",
    "df_new=pd.read_pickle('df_new_chkpt3.pkl')\n",
    "df_new=df_new[df_new['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    #In case I'm not loading this since the beginning\n",
    "    step_m=0.5\n",
    "    lat1,lat2,step_lat,lon1,lon2,step_lon,to_bin_lon,to_bin_lat = IL.build_grid(df_new,step_m)\n",
    "    number_lats,number_lon,df_new=IL.add_grid_todf(df_new,to_bin_lat,to_bin_lon,step_lat,step_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_coord_name= dict(zip(df_new.coords,df_new.name))\n",
    "dict_name_coord= dict(zip(df_new.name,df_new.coords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn about how groupby does not returns a dataframe but only groups the data IN ORDER TO APPLY SOME FUNCTION to it here: https://pandas.pydata.org/pandas-docs/stable/groupby.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp.reload(IL)\n",
    "Degenerate_loc, Degenerate_names, Degenerate_locbin, Degenerate_namesbin,Degenerate_freq = IL.name_id_degeneracy(df_new)\n",
    "Deg_names_dist = IL.pairwise_distances(Degenerate_loc,Degenerate_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many places have associated coordinates further apart than an arbitrary threshold t?\n",
    "# What are those places and what are their locations?\n",
    "\n",
    "t=.7 #threshold in km\n",
    "ct=0\n",
    "for d in Deg_names_dist:\n",
    "    if any( e>t for e in d[1]):\n",
    "        ct+=1\n",
    "        print d[0], '&', Degenerate_loc[Deg_names_dist.index(d)],'&', np.mean(d[1]),'\\\\\\\\'\n",
    "print ct,'places above this distance threshold in a list of ', len(Deg_names_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All locations associated with a degenerated place\n",
    "Degloc_all=[]\n",
    "for p in Degloc_copy:\n",
    "    for d in p:\n",
    "        Degloc_all.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link the locations associated to a place in particular to visualise what it means.\n",
    "m = folium.Map(location=[np.mean([c[0] for c in Degloc_all]),np.mean([c[1] for c in Degloc_all])], zoom_start=12)\n",
    "#for p in Degloc_all:\n",
    " #   folium.Marker(p).add_to(m)\n",
    "for p in Degloc_copy:\n",
    "    p.append(p[0])\n",
    "    folium.PolyLine(locations=p).add_to(m)\n",
    "m\n",
    "#m.save('places_hiN.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(IL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_fdate,dict_places_n=IL.first_apparition(df_new)\n",
    "    \n",
    "plt.figure()\n",
    "plt.scatter([n[1] for n in N_fdate],[n[0] for n in N_fdate])\n",
    "plt.xlabel('date of first apparition')\n",
    "plt.ylabel(r'$N_{occurences}$')\n",
    "#plt.savefig('N_data_first_apparition.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers=[]\n",
    "# Who are the outliers of the two last years (The places with a number of posts too high)\n",
    "# But not extremely high.\n",
    "for n in N_fdate:\n",
    "    if n[1].year>=2016:\n",
    "        if n[0]>=2000 and n [0]<10000:\n",
    "            Outliers.append((n[2],n[0]))\n",
    "    if n[1].year>=2017:\n",
    "        if n[0]>=1500 and n [0]<2000:\n",
    "            Outliers.append((n[2],n[0]))\n",
    "Outliers=[(e[0], e[1], dict_name_coord[e[0]]) for e in Outliers]\n",
    "Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Places_to_get=[]\n",
    "# How to access the points which are out of the general tendency (monuments, etc. ?)\n",
    "for k in dict_places_n.keys():\n",
    "    if dict_places_n[k]>10000:\n",
    "        Places_to_get.append([dict_name_coord[k],dict_places_n[k]])\n",
    "\n",
    "#Places_to_get is a list of [(lat,lon), N_post] for the places with a unusually high N_post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IL.plot_time_series(Places_to_get,dict_coord_name,df_new,'filename',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are the places that gather so much posts, on a map?\n",
    "m = folium.Map(location=[np.mean([c[0][0] for c in Places_to_get]),np.mean([c[0][1] for c in Places_to_get])], zoom_start=12)\n",
    "for p in Places_to_get:\n",
    "    folium.Marker(p[0]).add_to(m)\n",
    "m\n",
    "#m.save('places_hiN.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(set(df_new_nodup_cells.coords.tolist())), 'different coordinates in the dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_new_index,df_new_nodup2=IL.indexed_dataframe(df_new,step_lon,step_lat,number_lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make an index of only the years + months and text of posts\n",
    "df_new_index=df_new[['date','text','coordsbin']]\n",
    "df_new_index['date'] = pd.to_datetime(df_new_index['date'], errors='coerce')\n",
    "df_new_index['year_month'] = list(zip(df_new_index['date'].dt.year, df_new_index['date'].dt.month)) #add a month column\n",
    "yearmin=min([y[0] for y in df_new_index.year_month.tolist()])\n",
    "df_new_index['month_abs']=[(e[0]-yearmin)*12+e[1] for e in df_new_index.year_month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_new_index.shape\n",
    "print len(set(df_new_index.text.tolist()))\n",
    "print len(df_new_index.text.tolist())\n",
    "print len(set(df_new_index.coordsbin.tolist()))\n",
    "print len(df_new_index.coordsbin.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And pickle it\n",
    "#df_new_index.to_pickle('df_new_index_chkpt4.pkl')\n",
    "df_new_index=pd.read_pickle('df_new_index_chkpt4.pkl')\n",
    "df_new_index=df_new_index[df_new_index['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_years=list(set([e[0] for e in df_new_index.year_month.tolist()]))\n",
    "list_months=list(set([e[1] for e in df_new_index.year_month.tolist()]))\n",
    "print sorted(list_years)\n",
    "print sorted(list_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=datetime(2010,10,1)\n",
    "stop=datetime(2018,12,12)\n",
    "flags=IL.TimeFlags(start,stop,'months',1)\n",
    "slices=[(f,f+relativedelta(months=1)) for f in flags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My version NEW\n",
    "df_classified=IL.dataframe_classified(slices,df_new_index)\n",
    "df_classified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And pickle it\n",
    "#df_new.to_pickle('df_new_chkpt5.pkl')\n",
    "#df_new_index.to_pickle('df_new_index_chkpt5.pkl')\n",
    "#df_classified.to_pickle('df_classified_chkpt5.pkl')\n",
    "\n",
    "#df_new=pd.read_pickle('df_new_chkpt5.pkl')\n",
    "#df_new_index=pd.read_pickle('df_new_index_chkpt5.pkl')\n",
    "#df_classified=pd.read_pickle('df_classified_chkpt5.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    lat1,lat2,step_lat,lon1,lon2,step_lon,to_bin_lon,to_bin_lat = IL.build_grid(df_new,step_m)\n",
    "    number_lats,number_lon,df_new=IL.add_grid_todf(df_new,to_bin_lat,to_bin_lon,step_lat,step_lon)\n",
    "    print (number_lats, number_lon)\n",
    "    #Make an index of only the years + months and text of posts\n",
    "    gc.collect()\n",
    "    list_date=list(set([e.date() for e in df_new.date.tolist()]))\n",
    "    list_years=list(set([e.year for e in list_date]))\n",
    "    list_months=list(set([e.month for e in list_date]))\n",
    "    df_new_index=df_new[['date','text','coordsbin']]\n",
    "    df_new_index['date'] = pd.to_datetime(df_new_index['date'], errors='coerce')\n",
    "    df_new_index['year_month'] = list(zip(df_new_index['date'].dt.year, df_new_index['date'].dt.month)) #add a month column\n",
    "    yearmin=min([y[0] for y in df_new_index.year_month.tolist()])\n",
    "    df_new_index['month_abs']=[(e[0]-yearmin)*12+e[1] for e in df_new_index.year_month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's group the data by month and count how many coordsbin we have. \n",
    "\n",
    "# Group by month:\n",
    "count_Ncells_time=df_new_index[['month_abs','coordsbin']].set_index('coordsbin').groupby('month_abs')\n",
    "# Access the dictionary of groups (groups are the keys, coords are the values):\n",
    "dict_month_Ncells=count_Ncells_time.groups\n",
    "# For each month, let's count how many cells are filled.\n",
    "list_coords_Nposts=[]\n",
    "for month in dict_month_Ncells.keys():\n",
    "    coords=dict_month_Ncells[month]\n",
    "    # We want to keep only one occurence for each cell grid.\n",
    "    list_coords_Nposts.append((month, len(set(coords))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_Ncells=[IL.abs_to_yearmonth(m[0]) for m in list_coords_Nposts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(IL)\n",
    "for gridstep in [.5,1,3,5]:\n",
    "    plt.figure()\n",
    "    dates_Ncells_1, list_coords_Nposts_1 = IL.N_cells_info(df_new,gridstep)\n",
    "    plt.plot(dates_Ncells_1,[e[1] for e in list_coords_Nposts_1])\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel(r'$N_{cells}$ filled')\n",
    "    plt.savefig('N_gridcells_step='+str(gridstep)+'.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    steplon=gridstep/(40000*math.cos((lat1+lat2)*math.pi/360)/360)\n",
    "    steplat=gridstep/(40000./360.)\n",
    "\n",
    "for gridstep in [.5,1,3,5]:\n",
    "    lng=lon1\n",
    "    lonbins=[lng]\n",
    "    while lng<lon2+steplon:\n",
    "        lng+=steplon\n",
    "        lonbins.append(lng)\n",
    "\n",
    "    lat=lat1\n",
    "    latbins=[lat]\n",
    "    while lat<lat2+steplat:\n",
    "        lat+=steplat\n",
    "        latbins.append(lat)\n",
    "    continue\n",
    "        # Link the locations associated to a place in particular to visualise what it means.\n",
    "    m = folium.Map(location=[np.mean(latbins),np.mean(lonbins)], zoom_start=12)\n",
    "    for lng in lonbins:\n",
    "        folium.PolyLine(locations=((min(latbins),lng),(max(latbins),lng)),weight=1).add_to(m)\n",
    "    for lat in latbins:\n",
    "        folium.PolyLine(locations=((lat,min(lonbins)),(lat,max(lonbins))),weight=1).add_to(m)\n",
    "    #m.save('grid_'+str(gridstep)+'.html') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term vector building by NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming = removing all suffixes in the word to keep only its root.\n",
    "\n",
    "This part executes NMF on time slices of the data, to have an idea of what the output might be. It generates .mat files that can be used by matlab.\n",
    "\n",
    "More about NMF here: https://scikit-learn.org/stable/modules/decomposition.html#nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "french_stopwords = set(stopwords.words('french')) #le la les ou å pour etc.\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "italian_stopwords = set(stopwords.words('italian'))\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import nltk.data #\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "#Filter out the monuments?\n",
    "monuments=False\n",
    "all_stopwords=french_stopwords.union(english_stopwords)\n",
    "all_stopwords=all_stopwords.union(italian_stopwords)\n",
    "all_stopwords=all_stopwords.union(IL.words_added)\n",
    "all_stopwords=all_stopwords.union(IL.words_filtered)\n",
    "if monuments:\n",
    "    all_stopzords=all_stopwords.union(IL.words_monuments)\n",
    "    \n",
    "italian_stemmer = nltk.stem.SnowballStemmer('italian')\n",
    "list_reviews_all=df_classified.result.tolist()\n",
    "n_features=1000\n",
    "#Build the vectorizer and the list of all words in the dataset\n",
    "#df_new=pd.read_pickle('Tor_instagram_2.pkl')\n",
    "vectorizer_s = IL.StemmedCountVectorizer(max_df=0.95, max_features=n_features, analyzer=\"word\", stop_words=all_stopwords)\n",
    "while '' in list_reviews_all:\n",
    "    list_reviews_all.remove('')\n",
    "while \"\" in list_reviews_all:\n",
    "    list_reviews_all.remove(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info about countvectorizer here: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "And about TfIdf: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "\n",
    "And about NMF:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(df_classified, open('df_granularity.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts = vectorizer_s.fit_transform(list_reviews_all)\n",
    "#pickle.dump(counts, open('counts_master.pkl','wb'))\n",
    "#pickle.dump(vectorizer_s, open('vec_s_master.pkl','wb'))\n",
    "counts = pickle.load( open( 'counts_master.pkl', \"rb\" ) )\n",
    "vectorizer_s = pickle.load( open( 'vec_s_master.pkl', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the vectorizer with all the words in the dataset. Counts is the tweet/term matrix.\n",
    "# fit_transform: fit first (build the features list with the relevant words)\n",
    "# then transform: build the tweet/term matrix with the relevant tokens.\n",
    "\n",
    "vectorizer_new = StemmedCountVectorizer(max_df=0.95, max_features=n_features, analyzer=\"word\", stop_words=all_stopwords,vocabulary=vectorizer_s.vocabulary_) \n",
    "voc_vector={k:v for v,k in vectorizer_s.vocabulary_.iteritems()}\n",
    "voc_serie=pd.Series(voc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_new = StemmedCountVectorizer(max_df=0.95, max_features=n_features, analyzer=\"word\", stop_words=all_stopwords,vocabulary=vectorizer_s.vocabulary_) \n",
    "voc_vector={k:v for v,k in vectorizer_s.vocabulary_.iteritems()}\n",
    "voc_serie=pd.Series(voc_vector)\n",
    "#F = open('termvectors_sampling_5topics.txt','a') \n",
    "start=20\n",
    "n_topics=15\n",
    "for sample_run in xrange(1):\n",
    "    A,B,C,TermVectorsIndex,TermVectors = IL.NTF_sampling(False,df_classified,flags,0,0,vectorizer_new=vectorizer_new,n_topics=n_topics)\n",
    "    for tvi in TermVectorsIndex:\n",
    "        print TermVectorsIndex.index(tvi), [t[0] for t in tvi[0:8]]\n",
    "#        F.write(str([t[0] for t in tvi[0:10]]))\n",
    "#        F.write('\\n')\n",
    "#F.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topics definition\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.pcolormesh(B.T)#, cmap=plt.cm.Greys)\n",
    "plt.xlim(0,B.shape[0])\n",
    "plt.yticks(np.arange(B.shape[1])+0.5, np.arange(1,B.shape[1]+1))\n",
    "#plt.savefig('Topic_definition_normalized.pdf')\n",
    "plt.show()\n",
    "\n",
    "#for tvi in TermVectorsIndex:\n",
    "    #print TermVectorsIndex.index(tvi), [t for t in tvi[0:3]]\n",
    "#    print TermVectorsIndex.index(tvi), [t[0] for t in tvi[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CommonWords=set([])\n",
    "for t in TermVectors:\n",
    "    for t_other in TermVectors:\n",
    "        if t_other!=t:\n",
    "            for term in t.union(t_other):\n",
    "                CommonWords.add(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in TermVectors:\n",
    "    T=list(t)\n",
    "    print len(t)\n",
    "    for word in t:\n",
    "        if word in CommonWords:\n",
    "            T.remove(word)\n",
    "    t=set(T)\n",
    "    print len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[]\n",
    "for tvi in TermVectorsIndex:\n",
    "    #print TermVectorsIndex.index(tvi), [t for t in tvi[0:3]]\n",
    "    print TermVectorsIndex.index(tvi), [t[0].encode('utf-8') for t in tvi[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topics by bin\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.pcolormesh(A.T)#, cmap=plt.cm.Greys)\n",
    "plt.xlim(0,A.shape[0])\n",
    "plt.yticks(np.arange(B.shape[1])+0.5, np.arange(0,B.shape[1]+1))\n",
    "#plt.savefig('Topic_bybin_normalized.pdf')\n",
    "plt.show()\n",
    "\n",
    "for tvi in TermVectorsIndex[::-1]:\n",
    "    #print TermVectorsIndex.index(tvi), [t for t in tvi[0:3]]\n",
    "    print TermVectorsIndex.index(tvi), [t[0] for t in tvi[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(0,len(A.T)):\n",
    "    row=A.T[index]\n",
    "    print 'topic',index, '[',min(row),';', max(row)*100,']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geographical repartition of topics\n",
    "for index in range(0,len(A.T)):\n",
    "    row=A.T[index]\n",
    "    Coordinates=list(df_classified.index)\n",
    "    x=sorted(set([Cd[0] for Cd in Coordinates]))\n",
    "    y=sorted(set([Cd[1] for Cd in Coordinates]))\n",
    "    z=[(Coordinates[i], row[i]) for i in range(0,len(row))]\n",
    "    hem=np.zeros((len(x),len(y)))\n",
    "    for e in z:\n",
    "        hem[x.index(e[0][0])][y.index(e[0][1])]=e[1]\n",
    "    hmap = folium.Map(location=[np.mean(x), np.mean(y)], zoom_start=10, )\n",
    "\n",
    "    hm_wide = HeatMap( list(zip([C[0] for C in Coordinates],[C[1] for C in Coordinates],row)),\n",
    "                       min_opacity=0,\n",
    "                           max_val=max(row),\n",
    "                       radius=15, blur=0, \n",
    "                       max_zoom=1, \n",
    "                     )\n",
    "    hmap.add_child(hm_wide)\n",
    "\n",
    "    hmap.save('Heatmaps/heatmap_topic'+str(index)+'_normalized.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Topics Dynamics\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.pcolormesh([c[start:] for c in C.T])#, cmap=plt.cm.Greys)\n",
    "plt.xlim(0,len(C.T[0])-start)\n",
    "#plt.savefig('Topic_dynamic.pdf')\n",
    "plt.yticks(np.arange(n_topics)+0.5, np.arange(0,n_topics));\n",
    "#plt.xticks()\n",
    "plt.show()\n",
    "for tvi in TermVectorsIndex[::-1]:\n",
    "    #print TermVectorsIndex.index(tvi), [t for t in tvi[0:3]]\n",
    "    print TermVectorsIndex.index(tvi), [t[0] for t in tvi[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('sample_timelines')\n",
    "for index in range(0, len(C.T)):\n",
    "    row=C.T[index]\n",
    "    plt.figure()\n",
    "    plt.plot([abs_to_yearmonth(i) for i in range(0,len(row))], row)\n",
    "    plt.title('relative intensity of topic '+str(index)+' in a global monthly timeline')\n",
    "    plt.savefig('sample_timelines/plot_topic'+str(index)+'.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.DataFrame(A).groupby(voc_serie).sum()\n",
    "#dfa = dfa[dfa.index!='teachers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.pcolormesh(C.T)#, cmap=plt.cm.Greys)\n",
    "plt.xlim(0,C.shape[0])\n",
    "plt.yticks(np.arange(14)+0.5, np.arange(1,15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.io import savemat\n",
    "n_features=1000\n",
    "n_topics=10\n",
    "\n",
    "list_reviews_all=df_new['text'].tolist()\n",
    "counts = vectorizer_s.fit_transform(list_reviews_all)\n",
    "vectorizer_new = CountVectorizer(max_df=0.95, max_features=n_features,stop_words=all_stopwords_add,vocabulary=vectorizer_s.vocabulary_)\n",
    "ct=0\n",
    "\n",
    "# For each month in the dataset, get a snapshot view of the NMF tensor:\n",
    "for month in range(1,(len(list_years)-1)*12+2,3):\n",
    "    \n",
    "    year_start=list_years[0]+ int(month/12)\n",
    "    month_start=month%12  \n",
    "    year_end=list_years[0]+ int(month+11-0.5)/12\n",
    "    month_end=1+(month+11-1)%12\n",
    "    \n",
    "    try:\n",
    "        list_reviews_rest=dataframe_collection[str(year_start) + '-' + str(month_start)]['text'].tolist()\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "    # Learn the vocabulary dictionary and return term-document matrix.\n",
    "    counts = vectorizer_new.fit_transform(list_reviews_rest)\n",
    "    \n",
    "    #Transform a count matrix to a normalized tf-idf representation. \n",
    "    #(mainly =Terms with frequencies too hi or lo are removed)\n",
    "    tfidf = TfidfTransformer().fit_transform(counts)\n",
    "    \n",
    "    #Save the data in matlab friendly format\n",
    "    savemat('Inst500Matr' + str(ct), {'tfidf':tfidf})\n",
    "    \n",
    "    \n",
    "    nmf = decomposition.NMF(n_components=n_topics)\n",
    "    \n",
    "    #We are looking for two matrices whose product approximates tfidf.\n",
    "    \n",
    "    # Learn the model (parameters etc.). I.e \"take a look at what we have\". returns nothing.\n",
    "    W = nmf.fit(tfidf)\n",
    "    \n",
    "    # dictionary (factorization matrix)\n",
    "    H = nmf.components_\n",
    "    \n",
    "    # Extracts the data and performs NMF on the tfidf-treated dataset. \n",
    "    # Returns the matrix W (the weight matrix), to be multiplied by the matrix H (dictionary) to get target tfidf.\n",
    "    X2 = nmf.fit_transform(tfidf)\n",
    "    ct=ct+1\n",
    "    feature_names = vectorizer_new.get_feature_names()\n",
    "    print month\n",
    "    for i,text_topic in enumerate(X2.T):\n",
    "        \n",
    "        # i.e -> i, [content of column i of X2]\n",
    "        # i-th topic named text_topic\n",
    "        # Why sort them backwards?\n",
    "        text_top = text_topic.argsort()[:-21:-1]\n",
    "        word_top = nmf.components_[i].argsort()[:-11:-1]\n",
    "        \n",
    "        # For each text topic in the NMF resulting list, print the weight vector and the term vector.\n",
    "        print [nmf.components_[i,k] for k in nmf.components_[i].argsort()[:-11:-1]]\n",
    "        print [\", \".join([feature_names[i] for i in word_top])]\n",
    "        \n",
    "        # print the amount of non-zero components in the weight vector\n",
    "        l=[text_topic[i] for i in text_top]\n",
    "        print np.count_nonzero(l)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import numpy as np\n",
    ">>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    ">>> from sklearn.decomposition import NMF\n",
    ">>> model = NMF(n_components=2, init='random', random_state=0)\n",
    ">>> W = model.fit_transform(X)\n",
    ">>> H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(X):\n",
    "    print i,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the feature names in a csv file\n",
    "type(feature_names)\n",
    "import io\n",
    "\n",
    "f = io.open('test_inst_Torino.csv', 'w', encoding='utf8')\n",
    "for item in feature_names:\n",
    "    print item\n",
    "    f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boundaries of the map\n",
    "print (df_new['lonbin'].min()-step_lon/2., df_new['latbin'].min()-step_lat/2., df_new['lonbin'].max()+step_lon/2., df_new['latbin'].max()+step_lat/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_new_nodup_cells['n'].unique())\n",
    "list_rc=df_new_nodup2_cells['n'].tolist()\n",
    "with open('n_inst500.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s' % x for x in list_rc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nodup2_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_nodup2_cells.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "list_time=[]\n",
    "for month in range(1,(len(list_years)-1)*12+2,3):\n",
    "        year_start=list_years[0]+ int(month/12)\n",
    "        month_start=month%12  \n",
    "        year_end=list_years[0]+ int(month+11-0.5)/12\n",
    "        month_end=1+(month+5-1)%12\n",
    "             \n",
    "     \n",
    "        if month_end in([2,4,6,9,11]):\n",
    "            day=30\n",
    "        else:\n",
    "            day=31\n",
    "        deb=str(year_start) + '-' + str(month_end) + '-1'\n",
    "        deb_new=time.mktime(datetime.datetime.strptime(deb, \"%Y-%m-%d\").timetuple())\n",
    "        if month==1:\n",
    "            list_time= np.append(deb_new,list_time)\n",
    "        else:\n",
    "            list_time=np.append(list_time,deb_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print list_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('times_inst500.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join('%s' % x for x in list_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jupyG]",
   "language": "python",
   "name": "conda-env-jupyG-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
